{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Perform bag-of-words approach (count occurrence, normalized count occurrence), TF-IDF on\n",
        "data. Create embeddings using Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1MmZuTeu6-3",
        "outputId": "73248f42-36ce-424a-e498-9cf35f1b0e98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.4.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk gensim scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUO2IqyJu_KC",
        "outputId": "930f50e6-963d-4b9f-e3e4-09797f1a3885"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJitlP9ivA5g",
        "outputId": "a768cee4-c251-4911-bc24-a934ee0325a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset:\n",
            "1. The sun rises in the east\n",
            "2. The sun sets in the west\n",
            "3. It is the most important natural resource\n",
            "4. Life of earth would not have been possible without the sun\n"
          ]
        }
      ],
      "source": [
        "documents = [\n",
        "    \"The sun rises in the east\",\n",
        "    \"The sun sets in the west\",\n",
        "    \"It is the most important natural resource\",\n",
        "    \"Life of earth would not have been possible without the sun\"\n",
        "]\n",
        "\n",
        "print(\"Dataset:\")\n",
        "for i, doc in enumerate(documents, 1):\n",
        "    print(f\"{i}. {doc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgv-DZUHvG1i"
      },
      "source": [
        "# Bag of Words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kz2TF_tvDGW",
        "outputId": "67e9cab8-b870-4e96-c953-ef12e33879dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Vocabulary:\n",
            "['been' 'earth' 'east' 'have' 'important' 'in' 'is' 'it' 'life' 'most'\n",
            " 'natural' 'not' 'of' 'possible' 'resource' 'rises' 'sets' 'sun' 'the'\n",
            " 'west' 'without' 'would']\n",
            "\n",
            "Bag of Words Matrix (Count Occurrence):\n",
            "[[0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 2 0 0 0]\n",
            " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 2 1 0 0]\n",
            " [0 0 0 0 1 0 1 1 0 1 1 0 0 0 1 0 0 0 1 0 0 0]\n",
            " [1 1 0 1 0 0 0 0 1 0 0 1 1 1 0 0 0 1 1 0 1 1]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "bow_matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "print(\"\\nVocabulary:\")\n",
        "print(vectorizer.get_feature_names_out())\n",
        "\n",
        "print(\"\\nBag of Words Matrix (Count Occurrence):\")\n",
        "print(bow_matrix.toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XM9GEWtevNqR"
      },
      "source": [
        "# Normalized Bag of Words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j03zIzGVvI23",
        "outputId": "721bcf68-c2b1-4fc4-8bd9-5c7d1400c6eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Normalized Bag of Words Matrix:\n",
            "[[0.         0.         0.16666667 0.         0.         0.16666667\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.16666667 0.         0.16666667\n",
            "  0.33333333 0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.16666667\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.16666667 0.16666667\n",
            "  0.33333333 0.16666667 0.         0.        ]\n",
            " [0.         0.         0.         0.         0.14285714 0.\n",
            "  0.14285714 0.14285714 0.         0.14285714 0.14285714 0.\n",
            "  0.         0.         0.14285714 0.         0.         0.\n",
            "  0.14285714 0.         0.         0.        ]\n",
            " [0.09090909 0.09090909 0.         0.09090909 0.         0.\n",
            "  0.         0.         0.09090909 0.         0.         0.09090909\n",
            "  0.09090909 0.09090909 0.         0.         0.         0.09090909\n",
            "  0.09090909 0.         0.09090909 0.09090909]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "normalized_bow = normalize(bow_matrix, norm='l1')\n",
        "\n",
        "print(\"\\nNormalized Bag of Words Matrix:\")\n",
        "print(normalized_bow.toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLxoBWGtvRhH"
      },
      "source": [
        "# TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIPPiJYIvPx6",
        "outputId": "a719c21e-0c65-4e5d-da0f-d11d1a8a46c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "TF-IDF Vocabulary:\n",
            "['been' 'earth' 'east' 'have' 'important' 'in' 'is' 'it' 'life' 'most'\n",
            " 'natural' 'not' 'of' 'possible' 'resource' 'rises' 'sets' 'sun' 'the'\n",
            " 'west' 'without' 'would']\n",
            "\n",
            "TF-IDF Matrix:\n",
            "[[0.         0.         0.49276768 0.         0.         0.3885037\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.49276768 0.         0.31452723\n",
            "  0.51429323 0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.3885037\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.49276768 0.31452723\n",
            "  0.51429323 0.49276768 0.         0.        ]\n",
            " [0.         0.         0.         0.         0.39928771 0.\n",
            "  0.39928771 0.39928771 0.         0.39928771 0.39928771 0.\n",
            "  0.         0.         0.39928771 0.         0.         0.\n",
            "  0.20836489 0.         0.         0.        ]\n",
            " [0.32141667 0.32141667 0.         0.32141667 0.         0.\n",
            "  0.         0.         0.32141667 0.         0.         0.32141667\n",
            "  0.32141667 0.32141667 0.         0.         0.         0.2051561\n",
            "  0.16772855 0.         0.32141667 0.32141667]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
        "\n",
        "print(\"\\nTF-IDF Vocabulary:\")\n",
        "print(tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "print(\"\\nTF-IDF Matrix:\")\n",
        "print(tfidf_matrix.toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2GcCk9nvU_Q"
      },
      "source": [
        "# Word2Vec Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wt3JSqp_vTFQ",
        "outputId": "14d5504f-55fc-44d1-ce20-9deb6690021d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Vocabulary in Word2Vec:\n",
            "['the', 'sun', 'in', 'without', 'possible', 'been', 'have', 'not', 'would', 'earth', 'of', 'life', 'resource', 'natural', 'important', 'most', 'is', 'it', 'west', 'sets', 'east', 'rises']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Tokenize sentences\n",
        "tokenized_docs = [word_tokenize(doc.lower()) for doc in documents]\n",
        "\n",
        "# Train Word2Vec model\n",
        "w2v_model = Word2Vec(sentences=tokenized_docs, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "print(\"\\nVocabulary in Word2Vec:\")\n",
        "print(w2v_model.wv.index_to_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "es3l2U5jvYg-",
        "outputId": "89ce875b-0757-498c-bdf5-62e0f9456bb7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Word2Vec Embedding for 'sun':\n",
            " [-8.6212046e-03  3.6664882e-03  5.1911823e-03  5.7407771e-03\n",
            "  7.4679959e-03 -6.1686146e-03  1.1053376e-03  6.0487166e-03\n",
            " -2.8406929e-03 -6.1742957e-03 -4.1213547e-04 -8.3713057e-03\n",
            " -5.6016780e-03  7.1048136e-03  3.3528409e-03  7.2255796e-03\n",
            "  6.7999526e-03  7.5316946e-03 -3.7881434e-03 -5.6530442e-04\n",
            "  2.3505895e-03 -4.5175911e-03  8.3898595e-03 -9.8570567e-03\n",
            "  6.7651444e-03  2.9169703e-03 -4.9345442e-03  4.3962048e-03\n",
            " -1.7400645e-03  6.7118951e-03  9.9650640e-03 -4.3621953e-03\n",
            " -5.9824984e-04 -5.6966757e-03  3.8516975e-03  2.7874780e-03\n",
            "  6.8893526e-03  6.1014909e-03  9.5378626e-03  9.2714839e-03\n",
            "  7.8986809e-03 -6.9913771e-03 -9.1560846e-03 -3.5371701e-04\n",
            " -3.0998415e-03  7.8926133e-03  5.9367991e-03 -1.5476035e-03\n",
            "  1.5121385e-03  1.7903202e-03  7.8177862e-03 -9.5107406e-03\n",
            " -2.0507160e-04  3.4692646e-03 -9.3755248e-04  8.3820252e-03\n",
            "  9.0114186e-03  6.5351953e-03 -7.1143097e-04  7.7126590e-03\n",
            " -8.5349493e-03  3.2068391e-03 -4.6372004e-03 -5.0872229e-03\n",
            "  3.5906737e-03  5.3707273e-03  7.7698864e-03 -5.7640835e-03\n",
            "  7.4345097e-03  6.6256281e-03 -3.7089961e-03 -8.7449569e-03\n",
            "  5.4376968e-03  6.5104887e-03 -7.8526762e-04 -6.7088827e-03\n",
            " -7.0837969e-03 -2.4975985e-03  5.1442101e-03 -3.6656992e-03\n",
            " -9.3713803e-03  3.8262322e-03  4.8850812e-03 -6.4275777e-03\n",
            "  1.2079867e-03 -2.0734491e-03  2.6207228e-05 -9.8841311e-03\n",
            "  2.6921828e-03 -4.7503421e-03  1.0904957e-03 -1.5743222e-03\n",
            "  2.1990531e-03 -7.8824088e-03 -2.7173406e-03  2.6651900e-03\n",
            "  5.3476715e-03 -2.3914224e-03 -9.5101353e-03  4.5077908e-03]\n"
          ]
        }
      ],
      "source": [
        "word = \"sun\"\n",
        "vector = w2v_model.wv[word]\n",
        "print(f\"\\nWord2Vec Embedding for '{word}':\\n\", vector)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pu9U4pdnvaSt",
        "outputId": "5466f25a-24a7-42aa-f8ae-6fd98ff522f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Words similar to 'sun':\n",
            "important : 0.16075554490089417\n",
            "west : 0.1592675745487213\n",
            "natural : 0.13736103475093842\n",
            "most : 0.12309923022985458\n",
            "it : 0.08568026125431061\n",
            "without : 0.06793666630983353\n",
            "would : 0.03365974500775337\n",
            "is : 0.022437766194343567\n",
            "been : 0.009397289715707302\n",
            "life : 0.00835457257926464\n"
          ]
        }
      ],
      "source": [
        "similar_words = w2v_model.wv.most_similar(\"sun\")\n",
        "print(\"\\nWords similar to 'sun':\")\n",
        "for w, score in similar_words:\n",
        "    print(w, \":\", score)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
